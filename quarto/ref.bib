@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@misc{tensorflow,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@misc{keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}

@inproceedings{pytorch,
author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, CK and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Suo, Michael and Tillet, Phil and Wang, Eikan and Wang, Xiaodong and Wen, William and Zhang, Shunting and Zhao, Xu and Zhou, Keren and Zou, Richard and Mathews, Ajit and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
booktitle = {29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24)},
doi = {10.1145/3620665.3640366},
month = apr,
publisher = {ACM},
title = {{PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation}},
url = {https://pytorch.org/assets/pytorch2-2.pdf},
year = {2024}
}

@book{Minsky_Perceptrons,
    author = {Minsky, Marvin and Papert, Seymour A.},
    title = "{Perceptrons: An Introduction to Computational Geometry}",
    publisher = {The MIT Press},
    year = {2017},
    month = {09},
    abstract = "{The first systematic study of parallelism in computation by two pioneers in the field.Reissue of the 1988 Expanded Edition with a new foreword by Léon BottouIn 1969, ten years after the discovery of the perceptron—which showed that a machine could be taught to perform certain tasks using examples—Marvin Minsky and Seymour Papert published Perceptrons, their analysis of the computational capabilities of perceptrons for specific tasks. As Léon Bottou writes in his foreword to this edition, “Their rigorous work and brilliant technique does not make the perceptron look very good.” Perhaps as a result, research turned away from the perceptron. Then the pendulum swung back, and machine learning became the fastest-growing field in computer science. Minsky and Papert's insistence on its theoretical foundations is newly relevant.Perceptrons—the first systematic study of parallelism in computation—marked a historic turn in artificial intelligence, returning to the idea that intelligence might emerge from the activity of networks of neuron-like entities. Minsky and Papert provided mathematical analysis that showed the limitations of a class of computing machines that could be considered as models of the brain. Minsky and Papert added a new chapter in 1987 in which they discuss the state of parallel computers, and note a central theoretical challenge: reaching a deeper understanding of how “objects” or “agents” with individuality can emerge in a network. Progress in this area would link connectionism with what the authors have called “society theories of mind.”}",
    isbn = {9780262343930},
    doi = {10.7551/mitpress/11301.001.0001},
    url = {https://doi.org/10.7551/mitpress/11301.001.0001},
}

@ARTICLE{Lecun_CNN,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE},
  title={Gradient-based learning applied to document recognition},
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  keywords={Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis},
  doi={10.1109/5.726791}}


@misc{Simonyan_Deep_CNN,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition},
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      eprint={1409.1556},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1409.1556},
}

@misc{He_Deep_Residual_Learning,
      title={Deep Residual Learning for Image Recognition},
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1512.03385},
}

@misc{Szegedy_InceptionNet,
      title={Going Deeper with Convolutions},
      author={Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
      year={2014},
      eprint={1409.4842},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1409.4842},
}

@misc{kaggle_cancer,
    author = {Will Cukierski},
    title = {Histopathologic Cancer Detection},
    year = {2018},
    howpublished = {\url{https://kaggle.com/competitions/histopathologic-cancer-detection}},
    note = {Kaggle}
}

@misc{kaggle_nlp,
    author = {Addison Howard and devrishi and Phil Culliton and Yufeng Guo},
    title = {Natural Language Processing with Disaster Tweets},
    year = {2019},
    howpublished = {\url{https://kaggle.com/competitions/nlp-getting-started}},
    note = {Kaggle}
}

@book{py_lib_nltk,
author = {Bird, Steven and Klein, Ewan and Loper, Edward},
isbn = {9780596516499},
month = jun,
publisher = {O'Reilly Media, Inc.},
title = {{Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit}},
url = {https://www.nltk.org/book/},
year = {2009}
}

@article{LSTM_paper,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = {Long Short-Term Memory},
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}

@software{transformers_python_lib,
  author       = {Wolf, Thomas and
                  Debut, Lysandre and
                  Sanh, Victor and
                  Chaumond, Julien and
                  Delangue, Clement and
                  Moi, Anthony and
                  Cistac, Perric and
                  Ma, Clara and
                  Jernite, Yacine and
                  Plu, Julien and
                  Xu, Canwen and
                  Le Scao, Teven and
                  Gugger, Sylvain and
                  Drame, Mariama and
                  Lhoest, Quentin and
                  Rush, Alexander M.},
  title        = {{Transformers: State-of-the-Art Natural Language
                   Processing}},
  month        = dec,
  year         = 2022,
  publisher    = {Zenodo},
  version      = {v4.25.1},
  doi          = {10.5281/zenodo.7391177},
  url          = {https://doi.org/10.5281/zenodo.7391177}
}

@misc{kaggle_gan_monet,
    author = {Amy Jang and Ana Sofia Uzsoy and Phil Culliton},
    title = {I’m Something of a Painter Myself},
    year = {2020},
    howpublished = {\url{https://kaggle.com/competitions/gan-getting-started}},
    note = {Kaggle}
}

@misc{patch_gan,
      title={Image-to-Image Translation with Conditional Adversarial Networks},
      author={Phillip Isola and Jun-Yan Zhu and Tinghui Zhou and Alexei A. Efros},
      year={2018},
      eprint={1611.07004},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1611.07004},
}

@inproceedings{mifid,
   series={KDD ’21},
   title={On Training Sample Memorization: Lessons from Benchmarking Generative Modeling with a
       Large-scale Competition},
   url={http://dx.doi.org/10.1145/3447548.3467198},
   DOI={10.1145/3447548.3467198},
   booktitle={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data
       Mining},
   publisher={ACM},
   author={Bai, Ching-Yuan and Lin, Hsuan-Tien and Raffel, Colin and Kan, Wendy Chi-wen},
   year={2021},
   month=aug, collection={KDD ’21} }

@misc{ wiki:monet_nymph,
   author = {{Wikimedia Commons}},
   title = "File:Nympheas 71293 3.jpg --- Wikimedia Commons{,} the free media repository",
   year = "2024",
   url = "https://commons.wikimedia.org/w/index.php?title=File:Nympheas_71293_3.jpg&oldid=925007871",
   note = "[Online; accessed 6-December-2024]"
 }
