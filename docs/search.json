[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "",
    "text": "Generative Adversarial Networks (GANs) are a novel neural network architecture designed for generating synthetic data. Unlike traditional Convolutional Neural Networks (CNNs), which are designed for classification or feature extraction, GANs operate using a dual-network framework comprising a generator and a discriminator. The generator creates artificial outputs, while the discriminator evaluates their authenticity, driving iterative improvements in the generator’s output. This project applies GANs to transform photographic images into works of art inspired by Claude Monet, leveraging the distinctive style of an influential Impressionist artist.\n\n\n\n\n\n\nFigure 1: Water Lillies By Claude Monet circa 1915 from Wikimedia Commons (2024)\n\n\n\n\n\nThis projects creates a submission for the “I’m Something of a Painter Myself” Kaggle competition by Jang, Uzsoy, and Culliton (2020). The objective is to develop a GAN capable of generating Monet-style artworks from photographs. Model performance is evaluated using the Memorization-informed Fréchet Inception Distance (MiFID) metric Bai et al. (2021), which measures both generation quality and style transfer while preventing direct copying of training data.\n\n\n\nThis projects GAN implementation, detailed in Section 4, uses PyTorch (Ansel et al. 2024) with architectural features optimized for style transfer. The generator follows a multi-stage design: initial feature extraction, downsampling through strided convolutions, nine residual blocks (Szegedy et al. 2014) for style learning, and upsampling via transposed convolutions, ending with a tanh-activated output layer. The discriminator implements a PatchGAN (Isola et al. 2018) architecture, evaluating image authenticity at the patch level through progressive feature extraction.\n\n\n\nThe research workflow, shown in Figure 2, begins with Exploratory Data Analysis of the Monet and photographic image datasets. Following data preprocessing and GAN architecture development, we optimize the model by tuning learning rates, batch sizes, and architectural parameters. Training progress is tracked using generator loss, discriminator accuracy, and MiFID scores. The final model selection combines validation metrics and qualitative assessment to determine the best configuration for image generation.\n\n\n\n\n\n\n\nflowchart LR\n    Tune[\"&lt;div style='line-height:1.0;'&gt;Tune&lt;br&gt;Hyperparameters&lt;/div&gt;\"]\n\n    EDA[\"&lt;div style='line-height:1.0;'&gt;Exploratory&lt;br&gt;Data&lt;br&gt;Analysis&lt;/div&gt;\"]\n    --&gt; Clean[\"&lt;div style='line-height:1.0;'&gt;Clean&lt;br&gt;Original&lt;br&gt;Data&lt;/div&gt;\"]\n    --&gt; BuildModel[\"&lt;div style='line-height:1.0;'&gt;Build&lt;br&gt;GAN&lt;br&gt;Model&lt;/div&gt;\"]\n    --&gt; Train[\"&lt;div style='line-height:1.0;'&gt;Train&lt;br&gt;Model&lt;/div&gt;\"]\n    --&gt; OutputFinal[\"&lt;div style='line-height:1.0;'&gt;Final&lt;br&gt;Model&lt;/div&gt;\"]\n    --&gt; Submit[\"&lt;div style='line-height:1.0;'&gt;Submit&lt;br&gt;Results&lt;/div&gt;\"]\n\n    Train --&gt; Tune --&gt; Train\n\n\n\n\nFigure 2: GAN Project Workflow"
  },
  {
    "objectID": "index.html#kaggle-competition-specification",
    "href": "index.html#kaggle-competition-specification",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "",
    "text": "This projects creates a submission for the “I’m Something of a Painter Myself” Kaggle competition by Jang, Uzsoy, and Culliton (2020). The objective is to develop a GAN capable of generating Monet-style artworks from photographs. Model performance is evaluated using the Memorization-informed Fréchet Inception Distance (MiFID) metric Bai et al. (2021), which measures both generation quality and style transfer while preventing direct copying of training data."
  },
  {
    "objectID": "index.html#gan-architecture-overview",
    "href": "index.html#gan-architecture-overview",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "",
    "text": "This projects GAN implementation, detailed in Section 4, uses PyTorch (Ansel et al. 2024) with architectural features optimized for style transfer. The generator follows a multi-stage design: initial feature extraction, downsampling through strided convolutions, nine residual blocks (Szegedy et al. 2014) for style learning, and upsampling via transposed convolutions, ending with a tanh-activated output layer. The discriminator implements a PatchGAN (Isola et al. 2018) architecture, evaluating image authenticity at the patch level through progressive feature extraction."
  },
  {
    "objectID": "index.html#project-workflow",
    "href": "index.html#project-workflow",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "",
    "text": "The research workflow, shown in Figure 2, begins with Exploratory Data Analysis of the Monet and photographic image datasets. Following data preprocessing and GAN architecture development, we optimize the model by tuning learning rates, batch sizes, and architectural parameters. Training progress is tracked using generator loss, discriminator accuracy, and MiFID scores. The final model selection combines validation metrics and qualitative assessment to determine the best configuration for image generation.\n\n\n\n\n\n\n\nflowchart LR\n    Tune[\"&lt;div style='line-height:1.0;'&gt;Tune&lt;br&gt;Hyperparameters&lt;/div&gt;\"]\n\n    EDA[\"&lt;div style='line-height:1.0;'&gt;Exploratory&lt;br&gt;Data&lt;br&gt;Analysis&lt;/div&gt;\"]\n    --&gt; Clean[\"&lt;div style='line-height:1.0;'&gt;Clean&lt;br&gt;Original&lt;br&gt;Data&lt;/div&gt;\"]\n    --&gt; BuildModel[\"&lt;div style='line-height:1.0;'&gt;Build&lt;br&gt;GAN&lt;br&gt;Model&lt;/div&gt;\"]\n    --&gt; Train[\"&lt;div style='line-height:1.0;'&gt;Train&lt;br&gt;Model&lt;/div&gt;\"]\n    --&gt; OutputFinal[\"&lt;div style='line-height:1.0;'&gt;Final&lt;br&gt;Model&lt;/div&gt;\"]\n    --&gt; Submit[\"&lt;div style='line-height:1.0;'&gt;Submit&lt;br&gt;Results&lt;/div&gt;\"]\n\n    Train --&gt; Tune --&gt; Train\n\n\n\n\nFigure 2: GAN Project Workflow"
  },
  {
    "objectID": "index.html#sample-of-training-images",
    "href": "index.html#sample-of-training-images",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "2.1 Sample of Training Images",
    "text": "2.1 Sample of Training Images\n\n2.1.1 Monet Training Images\n\nCode\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\ntraining_monet_path = Path(\"../data/monet_jpg/\")\ntraining_monet_images = sorted(list(training_monet_path.rglob(\"*.jpg\")))\nselect_training_images = training_monet_images[:15]\n\ndef plot_images(img_paths, zoom=False, scale_factor=1):\n    for this_img in img_paths:\n        with Image.open(this_img) as img:\n            if zoom:\n                # Crop to the center 32x32 pixels\n                width, height = img.size\n                left = (width - 32) // 2\n                top = (height - 32) // 2\n                right = left + 32\n                bottom = top + 32\n                img = img.crop((left, top, right, bottom))\n\n            # Get the new image size\n            width, height = img.size\n\n            # Apply scale factor if zoomed\n            width *= scale_factor\n            height *= scale_factor\n\n            # Set figure size to match the scaled image size\n            dpi = 100  # Dots per inch\n            figsize = (width / dpi, height / dpi)\n\n            # Create figure and axes\n            fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n            ax.imshow(img)\n            ax.axis(\"off\")  # Turn off axes\n\n            # Remove all margins and display inline\n            plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n            plt.show()\n\nplot_images(select_training_images)\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n\n\n\n\n(g)\n\n\n\n\n\n\n\n\n\n\n\n(h)\n\n\n\n\n\n\n\n\n\n\n\n(i)\n\n\n\n\n\n\n\n\n\n\n\n(j)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(k)\n\n\n\n\n\n\n\n\n\n\n\n(l)\n\n\n\n\n\n\n\n\n\n\n\n(m)\n\n\n\n\n\n\n\n\n\n\n\n(n)\n\n\n\n\n\n\n\n\n\n\n\n(o)\n\n\n\n\n\n\n\nFigure 3: Sample of Artist Training Images\n\n\n\nIn Figure 3, we see a selection of Monet’s paintings. While they all share a distinctive style, there is noticeable variation in color palette, subject matter, and mood across the images.\n\n\n2.1.2 Photo Training Images\n\nCode\ntraining_photo_path = Path(\"../data/photo_jpg/\")\ntraining_photo_images = sorted(list(training_photo_path.rglob(\"*.jpg\")))\nselect_training_images = training_photo_images[:15]\nplot_images(select_training_images)\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n\n\n\n\n(g)\n\n\n\n\n\n\n\n\n\n\n\n(h)\n\n\n\n\n\n\n\n\n\n\n\n(i)\n\n\n\n\n\n\n\n\n\n\n\n(j)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(k)\n\n\n\n\n\n\n\n\n\n\n\n(l)\n\n\n\n\n\n\n\n\n\n\n\n(m)\n\n\n\n\n\n\n\n\n\n\n\n(n)\n\n\n\n\n\n\n\n\n\n\n\n(o)\n\n\n\n\n\n\n\nFigure 4: Sample of Photo Training Images\n\n\n\nIn Figure 3, there is a diverse collection of landscape photos, featuring a wide range of subjects, colors, seasons, and levels of detail."
  },
  {
    "objectID": "index.html#image-count",
    "href": "index.html#image-count",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "2.2 Image Count",
    "text": "2.2 Image Count\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\n\nsns.set_theme()\n\ndf = pd.DataFrame(\n    {\n        \"Name\": [\"Monet\", \"Photo\"],\n        \"Photo Count\": [len(training_monet_images), len(training_photo_images)],\n    }\n)\nplt.figure(figsize=(5, 2))\nax = sns.barplot(df, x=\"Name\", y=\"Photo Count\")\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.ylim(0, 8000)\n\n\n\n\n\n\n\n\nFigure 5: Image Count by Dataset\n\n\n\n\n\n\n\nCode\ndf\n\n\n\n\nTable 1: Count of Training Photos by Type\n\n\n\n\n\n\n\n\n\n\nName\nPhoto Count\n\n\n\n\n0\nMonet\n300\n\n\n1\nPhoto\n7038\n\n\n\n\n\n\n\n\n\n\nFigure 5 and Table 1 detail the count of training photos by type showing that there are 300 Monet-style images and 7,038 photographic images in the dataset."
  },
  {
    "objectID": "index.html#sec-color-stats",
    "href": "index.html#sec-color-stats",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "2.3 Image Color Statistics",
    "text": "2.3 Image Color Statistics\nTo statistically analyze the differences between the Monet and photographic image datasets, the mean and standard deviation of the red, green, and blue pixel values were calculated for each set. The Python code in Listing 1 reads each image, extracts the pixel values, and computes these statistics.\n\n\n\n\n\nListing 1: Image Color Statistics Computation\n\n\nimport numpy as np\n\ndef compute_mean_std(image_paths, dataset):\n    means = np.zeros(3)\n    stds = np.zeros(3)\n    for img_path in image_paths:\n        img = Image.open(img_path).convert(\"RGB\")\n        img = np.array(img) / 255.0  # Normalize to [0, 1]\n        means += img.mean(axis=(0, 1))  # Mean per channel (RGB)\n        stds += img.std(axis=(0, 1))  # Standard deviation per channel (RGB)\n\n    means /= len(image_paths)\n    stds /= len(image_paths)\n\n    result = []\n\n    color_map = {\n        0: \"Red\",\n        1: \"Green\",\n        2: \"Blue\",\n    }\n\n    for i, mean in enumerate(means):\n        result.append(\n            {\n                \"value\": mean,\n                \"color\": color_map[i],\n                \"dataset\": dataset,\n                \"measurement\": \"mean\",\n            }\n        )\n    for i, std in enumerate(stds):\n        result.append(\n            {\n                \"value\": std,\n                \"color\": color_map[i],\n                \"dataset\": dataset,\n                \"measurement\": \"std\",\n            }\n        )\n\n    return result\n\n\n\n\n\n\n\nCode\n# # Paths to the Monet images\n# monet_image_paths = training_monet_images\n# monet_stats = compute_mean_std(monet_image_paths, \"Monet\")\n\n# photo_image_paths = training_photo_images\n# photo_stats = compute_mean_std(photo_image_paths, \"Photo\")\n\n# train_stats = monet_stats + photo_stats\n# df = pd.DataFrame(train_stats)\n\n# df.to_parquet(\"image_stats.parquet\")\ndf = pd.read_parquet(\"image_stats.parquet\")\n\n\n\n\nCode\ndef plot_color_metric(this_df, color, metric):\n    plt.figure(figsize=(2.5, 1.5))\n    this_df = this_df[this_df['color'] == color]\n    ax = sns.barplot(this_df[this_df['measurement'] == metric], x=\"color\", y=\"value\", hue=\"dataset\")\n    for container in ax.containers:\n        ax.bar_label(container, fmt=\"%.2f\")\n    plt.legend(\n        title=\"Dataset\",\n        loc=\"upper center\",  # Center the legend horizontally\n        bbox_to_anchor=(0.5, -0.25),  # Adjust the vertical position below the plot\n        ncol=3,  # Display the legend in two columns (optional for compactness)\n        frameon=False,  # Remove the legend border (optional)\n    )\n    plt.xlabel(None)\n    plt.ylabel(metric.title())\n    plt.show()\n\nplot_color_metric(df, \"Red\", \"mean\")\nplot_color_metric(df, \"Green\", \"mean\")\nplot_color_metric(df, \"Blue\", \"mean\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Red\n\n\n\n\n\n\n\n\n\n\n\n(b) Green\n\n\n\n\n\n\n\n\n\n\n\n(c) Blue\n\n\n\n\n\n\n\nFigure 6: Image Mean Colors by Dataset\n\n\n\n\nCode\nplot_color_metric(df, \"Red\", \"std\")\nplot_color_metric(df, \"Green\", \"std\")\nplot_color_metric(df, \"Blue\", \"std\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Red\n\n\n\n\n\n\n\n\n\n\n\n(b) Green\n\n\n\n\n\n\n\n\n\n\n\n(c) Blue\n\n\n\n\n\n\n\nFigure 7: Image Standard Deviation Colors by Dataset\n\n\n\n\nThe computed mean and standard deviation for each color channel (Red, Green, Blue) are shown in Figure 6 and Figure 7. From these statistics, we observe that the Monet dataset has higher mean values for the red and green channels compared to the photo dataset, suggesting that on average the photo dataset is darker than the Monet dataset. For standard deviations, the Monet dataset exhibits lower variation in color channels compared to the photographic images, suggesting a more consistent color scheme across Monet-style paintings."
  },
  {
    "objectID": "index.html#image-normalization-implementation",
    "href": "index.html#image-normalization-implementation",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "3.1 Image Normalization Implementation",
    "text": "3.1 Image Normalization Implementation\nThe code in Listing 2 defines the normalization process for both the Monet and photo datasets. The monet_transform and photo_transform functions use the respective means and standard deviations calculated earlier to normalize the images. By applying these transformations during data loading, the images are appropriately adjusted to ensure consistency across the datasets.\n\n\n\n\n\nListing 2: Image Normalization Implementation\n\n\nmonet_transform = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Normalize(mean=monet_means, std=monet_stds),\n    ]\n)\nphoto_transform = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Normalize(mean=photo_means, std=photo_stds),\n    ]\n)\n\nphoto_dataset = ImageDataset(\n    root_dir=Path(data_dir, \"photo_jpg\"), transform=photo_transform\n)\nmonet_dataset = ImageDataset(\n    root_dir=Path(data_dir, \"monet_jpg\"), transform=monet_transform\n)\n\nphoto_loader = DataLoader(photo_dataset, batch_size=1, shuffle=False)\nmonet_loader = DataLoader(monet_dataset, batch_size=1, shuffle=False)"
  },
  {
    "objectID": "index.html#data-cleaning-considerations",
    "href": "index.html#data-cleaning-considerations",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "3.2 Data Cleaning Considerations",
    "text": "3.2 Data Cleaning Considerations\nThe process of normalizing the images helps address any outliers or extreme values in the pixel data, ensuring that both datasets have a comparable color range. This step not only aids in reducing inconsistencies but also facilitates more stable model training by ensuring the model focuses on learning relevant features rather than being influenced by differences in color distributions. Standardizing the color values in each dataset also helps remove any potential biases in the learning process, allowing the model to generate more accurate and Monet-style images."
  },
  {
    "objectID": "index.html#gan-architecture",
    "href": "index.html#gan-architecture",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "4.1 GAN Architecture",
    "text": "4.1 GAN Architecture"
  },
  {
    "objectID": "index.html#generator",
    "href": "index.html#generator",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "4.2 Generator",
    "text": "4.2 Generator\nThe Generator network, implemented in Listing 3, is responsible for translating input photographic images into Monet-style paintings. It is composed of several stages:\n\nInitial Feature Extraction:\n\nThe generator starts with a convolutional layer that maps the input RGB image to a higher-dimensional feature space. This is followed by an instance normalization layer and a ReLU activation function. These steps help preserve contrast and style information from the original image while facilitating feature extraction.\n\nDownsampling Layers:\n\nTwo convolutional layers with a stride of 2 progressively downsample the input, doubling the number of channels at each step. These layers enable the network to learn more abstract, high-level features of the image while reducing computational complexity.\n\nResidual Blocks:\n\nNine ResidualBlock modules form the core of the generator. Each block consists of two convolutional layers with instance normalization and ReLU activation. A key feature of these blocks is the skip connection, where the input to each block is added to its output. This encourages the model to focus on learning modifications to the input rather than creating entirely new representations, improving training stability and generalization.\n\nUpsampling Layers:\n\nThe network uses transposed convolutional layers to upsample the image back to its original dimensions. These layers halve the number of channels at each step while maintaining spatial coherence.\n\nOutput Layer:\n\nA final convolutional layer with a tanh activation produces the output image. The tanh activation function ensures that pixel values are normalized between -1 and 1, suitable for further processing and visual output.\n\n\n\n\n\n\nListing 3: GAN Model Generator\n\n\nimport torch.nn as nn\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(\n                channels, channels, kernel_size=3, stride=1, padding=1, bias=False\n            ),\n            nn.InstanceNorm2d(channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(\n                channels, channels, kernel_size=3, stride=1, padding=1, bias=False\n            ),\n            nn.InstanceNorm2d(channels),\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\nclass Generator(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3, num_residual_blocks=9):\n        super(Generator, self).__init__()\n        model = [\n            nn.Conv2d(in_channels, 64, kernel_size=7, stride=1, padding=3, bias=False),\n            nn.InstanceNorm2d(64),\n            nn.ReLU(inplace=True),\n        ]\n\n        # Downsampling\n        in_features = 64\n        for _ in range(2):\n            out_features = in_features * 2\n            model += [\n                nn.Conv2d(\n                    in_features,\n                    out_features,\n                    kernel_size=3,\n                    stride=2,\n                    padding=1,\n                    bias=False,\n                ),\n                nn.InstanceNorm2d(out_features),\n                nn.ReLU(inplace=True),\n            ]\n            in_features = out_features\n\n        # Residual Blocks\n        for _ in range(num_residual_blocks):\n            model += [ResidualBlock(in_features)]\n\n        # Upsampling\n        for _ in range(2):\n            out_features = in_features // 2\n            model += [\n                nn.ConvTranspose2d(\n                    in_features,\n                    out_features,\n                    kernel_size=3,\n                    stride=2,\n                    padding=1,\n                    output_padding=1,\n                    bias=False,\n                ),\n                nn.InstanceNorm2d(out_features),\n                nn.ReLU(inplace=True),\n            ]\n            in_features = out_features\n\n        # Output Layer\n        model += [\n            nn.Conv2d(in_features, out_channels, kernel_size=7, stride=1, padding=3),\n            nn.Tanh(),\n        ]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        return self.model(x)"
  },
  {
    "objectID": "index.html#discriminator",
    "href": "index.html#discriminator",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "4.3 Discriminator",
    "text": "4.3 Discriminator\nThe Discriminator network, implemented in Listing 4, is designed to distinguish between real Monet paintings and the generated images. It follows a PatchGAN structure developed by Isola et al. (2018), which classifies small image patches instead of the entire image. This approach allows the discriminator to focus on local style consistency. The discriminator consists of the following stages:\n\nProgressive Feature Extraction:\n\nThe discriminator uses successive convolutional layers with increasing filter sizes and decreasing strides. After each convolution, instance normalization and leaky ReLU activations are applied to ensure stability and enhance the network’s ability to discriminate between real and fake images.\n\nFinal Classification:\n\nThe output is reduced to a single-channel map using a final convolutional layer. This map represents the authenticity of each patch in the input image, with values close to 1 indicating real Monet paintings and values close to 0 indicating fake images.\n\n\n\n\n\n\nListing 4: GAN Model Discriminator\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, in_channels=3):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, stride):\n            return [\n                nn.Conv2d(\n                    in_filters,\n                    out_filters,\n                    kernel_size=4,\n                    stride=stride,\n                    padding=1,\n                    bias=False,\n                ),\n                nn.InstanceNorm2d(out_filters),\n                nn.LeakyReLU(0.2, inplace=True),\n            ]\n\n        self.model = nn.Sequential(\n            *discriminator_block(in_channels, 64, stride=2),\n            *discriminator_block(64, 128, stride=2),\n            *discriminator_block(128, 256, stride=2),\n            *discriminator_block(256, 512, stride=1),\n            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1),\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n\n\n\nThe combination of these two neural networks, the Generator and Discriminator, creates a framework for generating Monet-style artwork from photographic images. The generator produces realistic paintings, while the discriminator ensures the authenticity of the generated images."
  },
  {
    "objectID": "index.html#training-setup",
    "href": "index.html#training-setup",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "5.1 Training Setup",
    "text": "5.1 Training Setup\nThe training setup initializes the generator and discriminator networks, defines optimizers for each model, and sets the loss functions critical to the GAN framework.\n\n\n\n\nListing 5: Initialize Models\n\n\n# Initialize Models\nG = Generator().to(device)  # Photo -&gt; Monet\nF = Generator().to(device)  # Monet -&gt; Photo\nD_M = Discriminator().to(device)  # Monet Discriminator\nD_P = Discriminator().to(device)  # Photo Discriminator\n\n\n\n\n\nGenerators (G and F): The generator G transforms photos to Monet-style paintings, while F performs the inverse transformation.\nDiscriminators (D_M and D_P): These networks evaluate the realism of Monet and photo images, providing feedback to their respective generators.\n\n\n\n\n\nListing 6: Initialize Optimizers\n\n\n# Optimizers\nlr = 0.0002\nG_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\nF_optimizer = optim.Adam(F.parameters(), lr=lr, betas=(0.5, 0.999))\nD_M_optimizer = optim.Adam(D_M.parameters(), lr=lr, betas=(0.5, 0.999))\nD_P_optimizer = optim.Adam(D_P.parameters(), lr=lr, betas=(0.5, 0.999))\n\n\n\n\n\nLearning Rate: A small learning rate of 0.0002 ensures gradual optimization.\nAdam Optimizer: With momentum parameters (\\beta_1=0.5, \\beta_2=0.999), it balances convergence and stability.\n\n\n\n\n\nListing 7: Initialize Loss Functions\n\n\n# Loss Functions\nadversarial_loss = nn.MSELoss()\ncycle_loss = nn.L1Loss()\nidentity_loss = nn.L1Loss()\n\n\n\n\n\nAdversarial Loss: Encourages the generators to produce images indistinguishable from real ones.\nCycle Consistency Loss: Ensures that translating a photo to Monet style and back reconstructs the original photo.\nIdentity Loss: Encourages generators to maintain content identity when inputs are already in the target domain."
  },
  {
    "objectID": "index.html#training-loop",
    "href": "index.html#training-loop",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "5.2 Training Loop",
    "text": "5.2 Training Loop\nThe training loop alternates between updating the generators and the discriminators, ensuring they improve together in a competitive framework.\n\n\n\n\nListing 8: Training Loop - Loading Data\n\n\nepochs = 250\n\nfor epoch in range(1, epochs + 1):\n    for i, (photo_batch, monet_batch) in enumerate(zip(photo_loader, monet_loader)):\n        # Load Data\n        real_photo = photo_batch[0].to(device)\n        real_monet = monet_batch[0].to(device)\n\n\n\n\n\nData Loading: Photo and Monet batches are fed into the model, converted to tensors, and sent to the GPU.\n\n\n\n\n\nListing 9: Training Loop - Cycle Translations\n\n\n        # Step 1: Update Generators (G and F)\n        G_optimizer.zero_grad()\n        F_optimizer.zero_grad()\n\n        # Photo -&gt; Monet -&gt; Photo (Cycle 1)\n        fake_monet = G(real_photo)\n        reconstructed_photo = F(fake_monet)\n\n        # Monet -&gt; Photo -&gt; Monet (Cycle 2)\n        fake_photo = F(real_monet)\n        reconstructed_monet = G(fake_photo)\n\n\n\n\n\nCycle Translations: Photos are converted to Monet-style and back to photos, and vice versa.\n\n\n\n\n\nListing 10: Training Loop - Generator Adversarial Loss\n\n\n        # Adversarial Loss\n        valid = torch.ones_like(D_M(fake_monet))\n        g_loss_monet = adversarial_loss(D_M(fake_monet), valid)\n\n        valid = torch.ones_like(D_P(fake_photo))\n        g_loss_photo = adversarial_loss(D_P(fake_photo), valid)\n\n\n\n\n\nGenerator Adversarial Loss: Measures how well the generator fools the discriminator.\n\n\n\n\n\nListing 11: Training Loop - Cycle and Identity Loss\n\n\n        # Cycle Consistency Loss\n        cycle_loss_photo = cycle_loss(reconstructed_photo, real_photo)\n        cycle_loss_monet = cycle_loss(reconstructed_monet, real_monet)\n\n        # Identity Loss\n        identity_photo = identity_loss(F(real_photo), real_photo)\n        identity_monet = identity_loss(G(real_monet), real_monet)\n\n\n\n\n\nCycle Loss: Ensures that the output preserves the original content through transformations.\nIdentity Loss: Maintains domain consistency for already-translated images.\n\n\n\n\n\nListing 12: Training Loop - Total Loss\n\n\n        # Total Generator Loss\n        total_g_loss = (\n            g_loss_monet\n            + g_loss_photo\n            + 10 * (cycle_loss_photo + cycle_loss_monet)\n            + 5 * (identity_photo + identity_monet)\n        )\n        total_g_loss.backward()\n        G_optimizer.step()\n        F_optimizer.step()\n\n\n\n\n\nWeighted Losses: Combines adversarial, cycle consistency, and identity losses into a total loss for optimization."
  },
  {
    "objectID": "index.html#image-generation-from-trained-model",
    "href": "index.html#image-generation-from-trained-model",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "5.3 Image Generation from Trained Model",
    "text": "5.3 Image Generation from Trained Model\nAfter training, the generator is used to transform unseen photos into Monet-style images.\n\n\n\n\nListing 13: Training Loop - Total Loss\n\n\nG.eval()\nwith torch.no_grad():\n    for i, (photo, _) in enumerate(photo_loader):\n        photo = photo.to(device)\n        fake_monet = G(photo)\n        fake_monet = (fake_monet + 1) / 2  # Denormalize to [0, 1]\n        fake_monet = transforms.ToPILImage()(fake_monet.squeeze().cpu())\n        fake_monet.save(os.path.join(output_dir, f\"image_{i + 1:04d}.jpg\"))\n\n\n\n\n\nEvaluation Mode: Switches the generator to inference mode to avoid gradient computation.\nDenormalization: Converts output values from [-1, 1] to [0, 1] for image representation."
  },
  {
    "objectID": "index.html#training-metrics",
    "href": "index.html#training-metrics",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "5.4 Training Metrics",
    "text": "5.4 Training Metrics\nThe figures below detail the statistics measured during training. Normalized vs non normalized training results are compared.\n\n\nCode\ndata_epoch = 250\ndf = pd.read_parquet(f\"../training_stats/training_stats.monet_transform.{data_epoch}_of_250_epochs.parquet\")\ndf = df.set_index(['Epoch'])\n\nalt_epoch = 150\ndf_alt = pd.read_parquet(f\"../training_stats/training_stats.no_transform.{alt_epoch}_of_250_epochs.parquet\")\ndf_alt = df_alt.set_index(['Epoch'])\n# df_alt = df_alt.add_prefix(\"No Transform \")\n\n# df = pd.concat([df, df_alt], axis=\"index\")\nfigsize = (4.5, 2.5)\n\n\n\n\nCode\ncol = 'Generator Total Loss'\n\ndf[[col]].plot(figsize=figsize, ylabel=\"Generator Total Loss\")\ndf_alt[[col]].plot(figsize=figsize, ylabel=\"Generator Total Loss\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Normalized Transform\n\n\n\n\n\n\n\n\n\n\n\n(b) No Transform\n\n\n\n\n\n\n\nFigure 8: Training: Generator Total Loss\n\n\n\n\nCode\ndf[['Monet Discriminator Loss', \"Photo Discriminator Loss\"]].plot(figsize=figsize, ylabel=\"Discriminator Loss\")\ndf_alt[['Monet Discriminator Loss', \"Photo Discriminator Loss\"]].plot(figsize=figsize, ylabel=\"Discriminator Loss\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Normalized Transform\n\n\n\n\n\n\n\n\n\n\n\n(b) No Transform\n\n\n\n\n\n\n\nFigure 9: Training: Discriminator Loss\n\n\n\n\nCode\ndf[['Identity Loss Photo', \"Identity Loss Monet\"]].plot(figsize=figsize, ylabel=\"Identity Loss\")\ndf_alt[['Identity Loss Photo', \"Identity Loss Monet\"]].plot(figsize=figsize, ylabel=\"Identity Loss\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Normalized Transform\n\n\n\n\n\n\n\n\n\n\n\n(b) No Transform\n\n\n\n\n\n\n\nFigure 10: Training: Identity Loss\n\n\n\n\nCode\ndf[['Cycle Consistency Loss Photo', \"Cycle Consistency Loss Monet\"]].plot(figsize=figsize, ylabel=\"Cycle Consistency Loss\")\ndf_alt[['Cycle Consistency Loss Photo', \"Cycle Consistency Loss Monet\"]].plot(figsize=figsize, ylabel=\"Cycle Consistency Loss\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Normalized Transform\n\n\n\n\n\n\n\n\n\n\n\n(b) No Transform\n\n\n\n\n\n\n\nFigure 11: Training: Cycle Consistency Loss\n\n\n\n\nCode\ndf[['Total Loss']].plot(figsize=figsize, ylabel=\"Total Loss\")\ndf_alt[['Total Loss']].plot(figsize=figsize, ylabel=\"Total Loss\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Normalized Transform\n\n\n\n\n\n\n\n\n\n\n\n(b) No Transform\n\n\n\n\n\n\n\nFigure 12: Training: Total Loss\n\n\n\n\nCode\ndf[['Epoch Time (s)']].plot(figsize=figsize, ylabel=\"Compute Time [s]\")\ndf_alt[['Epoch Time (s)']].plot(figsize=figsize, ylabel=\"Compute Time [s]\")\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Normalized Transform\n\n\n\n\n\n\n\n\n\n\n\n(b) No Transform\n\n\n\n\n\n\n\nFigure 13: Training: Computation Time Per Epoch in Seconds\n\n\n\n\nTraining between the two transform methods looks comparable. Both models reduce loss during training and appear to stabilize."
  },
  {
    "objectID": "index.html#comparison-between-non-normalized-and-normalized-output-images",
    "href": "index.html#comparison-between-non-normalized-and-normalized-output-images",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "6.1 Comparison Between Non Normalized and Normalized Output Images",
    "text": "6.1 Comparison Between Non Normalized and Normalized Output Images\n\n6.1.1 Epoch 25\n\nCode\nplot_nt_epoch_images(25)\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\nFigure 14: Non Normalized Sample of Output Images from Epoch 25\n\n\n\n\nCode\nplot_epoch_images(25)\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\nFigure 15: Normalized Sample of Output Images from Epoch 25\n\n\n\n\n\n6.1.2 Epoch 100\n\nCode\nplot_nt_epoch_images(100)\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\nFigure 16: Non Normalized Sample of Output Images from Epoch 100\n\n\n\n\nCode\nplot_epoch_images(100)\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\nFigure 17: Normalized Sample of Output Images from Epoch 100\n\n\n\nIf Figure 14 vs. Figure 15 there are significant visual differences in the magnitude of colors. The non normalized images are much more faint. This trend continues through epoch 100 with the non normalized images appearing to have some of the Monet style, but problems with color."
  },
  {
    "objectID": "index.html#normalized-model",
    "href": "index.html#normalized-model",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "6.2 Normalized Model",
    "text": "6.2 Normalized Model\n\nCode\nplot_epoch_images(5)\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\nFigure 18: Sample of Output Images from Epoch 5\n\n\n\n\nCode\nplot_epoch_images(25)\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\nFigure 19: Sample of Output Images from Epoch 25\n\n\n\n\nCode\nplot_epoch_images(50)\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\nFigure 20: Sample of Output Images from Epoch 50\n\n\n\n\nCode\nplot_epoch_images(75)\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\nFigure 21: Sample of Output Images from Epoch 75\n\n\n\n\nCode\nplot_epoch_images(100)\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\nFigure 22: Sample of Output Images from Epoch 100\n\n\n\n\nCode\nplot_epoch_images(150)\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\nFigure 23: Sample of Output Images from Epoch 150\n\n\n\n\nCode\nplot_epoch_images(200)\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\nFigure 24: Sample of Output Images from Epoch 200\n\n\n\n\nCode\nplot_epoch_images(225)\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\nFigure 25: Sample of Output Images from Epoch 225\n\n\n\n\nCode\nplot_epoch_images(250)\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\nFigure 26: Sample of Output Images from Epoch 250\n\n\n\nThese normalized images show progress in training. The images from 5 to 50 epochs appear to be learning the Monet style and the images from epoch 75 - 225 strengthen the style. At epoch 250 the output images darken and lose their detail, most likely due to overfitting."
  },
  {
    "objectID": "index.html#compare-single-image",
    "href": "index.html#compare-single-image",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "6.3 Compare Single Image",
    "text": "6.3 Compare Single Image\n\n\nCode\nepochs = [25, 50, 75, 100, 150, 200, 225, 250]\n\n# bb15fcc4ff.jpg\ninput_image = Path(\"../data/photo_jpg/\")\ninput_paths = list(input_image.rglob(\"*.jpg\"))\nplot_images([input_paths[1]])\n\ndef plot_image_over_epoch(epochs, index):\n    for epoch in epochs:\n        epoch_path = Path(f\"../output_images/monet_transform.{epoch}_epochs/\")\n        epoch_images = sorted(list(epoch_path.rglob(\"*.jpg\")))\n        select_epoch_images = epoch_images[index-1]\n        plot_images([select_epoch_images])\n\nplot_image_over_epoch(epochs, 2)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Input Image\n\n\n\n\n\n\n\n\n\n\n\n(b) 25 Epochs\n\n\n\n\n\n\n\n\n\n\n\n(c) 50 Epochs\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) 75 Epochs\n\n\n\n\n\n\n\n\n\n\n\n(e) 100 Epochs\n\n\n\n\n\n\n\n\n\n\n\n(f) 150 Epochs\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g) 200 Epochs\n\n\n\n\n\n\n\n\n\n\n\n(h) 225 Epochs\n\n\n\n\n\n\n\n\n\n\n\n(i) 250 Epochs\n\n\n\n\n\n\n\nFigure 27: Image 2 During Training\n\n\n\nIn the single figure comparison in Figure 27 we observe a progression from a single input image through training epoch 250. Initially the model lacks some stylistic details, but around 75 epochs the model starts to match the Monet style. Epochs 100 to 225 strengthen this style up until the model starts to overfit at epoch 250."
  },
  {
    "objectID": "index.html#kaggle-scores",
    "href": "index.html#kaggle-scores",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "6.4 Kaggle Scores",
    "text": "6.4 Kaggle Scores\n\n\nCode\nkaggle_scores = [\n    {\n        \"Version\": 1,\n        \"Epoch\": 50,\n        \"Score\": 77.69411,\n        \"Transform\": \"Normalized\",\n    },\n    {\n        \"Version\": 2,\n        \"Epoch\": 25,\n        \"Score\": 93.17964,\n        \"Transform\": \"Normalized\",\n    },\n    {\n        \"Version\": 3,\n        \"Epoch\": 75,\n        \"Score\": 84.96981,\n        \"Transform\": \"Normalized\",\n    },\n    {\n        \"Version\": 4,\n        \"Epoch\": 100,\n        \"Score\": 70.22760,\n        \"Transform\": \"Normalized\",\n    },\n    {\n        \"Version\": 5,\n        \"Epoch\": 150,\n        \"Score\": 58.26310,\n        \"Transform\": \"Normalized\",\n    },\n    {\n        \"Version\": 6,\n        \"Epoch\": 200,\n        \"Score\": 53.81515,\n        \"Transform\": \"Normalized\",\n    },\n    {\n        \"Version\": 7,\n        \"Epoch\": 175,\n        \"Score\": 54.78088,\n        \"Transform\": \"Normalized\",\n    },\n    {\n        \"Version\": 8,\n        \"Epoch\": 225,\n        \"Score\": 56.20738,\n        \"Transform\": \"Normalized\",\n    },\n    # {\n    #     \"Version\": 9,\n    #     \"Epoch\": 225,\n    #     \"Score\": 56.20738,\n    # },\n    {\n        \"Version\": 10,\n        \"Epoch\": 235,\n        \"Score\": 55.86135,\n        \"Transform\": \"Normalized\",\n    },\n    {\n        \"Version\": 11,\n        \"Epoch\": 50,\n        \"Score\": 69.20071,\n        \"Transform\": \"None\",\n    },\n    {\n        \"Version\": 12,\n        \"Epoch\": 25,\n        \"Score\": 88.95808,\n        \"Transform\": \"None\",\n    },\n    {\n        \"Version\": 13,\n        \"Epoch\": 75,\n        \"Score\": 68.87041,\n        \"Transform\": \"None\",\n    },\n    {\n        \"Version\": 14,\n        \"Epoch\": 100,\n        \"Score\": 66.95614,\n        \"Transform\": \"None\",\n    },\n    {\n        \"Version\": 15,\n        \"Epoch\": 150,\n        \"Score\": 69.73371,\n        \"Transform\": \"None\",\n    },\n]\n\ndf = pd.DataFrame(kaggle_scores)\ndf = df.sort_values([\"Epoch\"])\n\n\n\n\nCode\ndf = pd.DataFrame(kaggle_scores)\ndf = df.sort_values(['Version'])\ndf = df.set_index([\"Version\"])\ndf\n\n\n\n\nTable 2: Kaggle Results by Kaggle Submission Version\n\n\n\n\n\n\n\n\n\n\nEpoch\nScore\nTransform\n\n\nVersion\n\n\n\n\n\n\n\n1\n50\n77.69411\nNormalized\n\n\n2\n25\n93.17964\nNormalized\n\n\n3\n75\n84.96981\nNormalized\n\n\n4\n100\n70.22760\nNormalized\n\n\n5\n150\n58.26310\nNormalized\n\n\n6\n200\n53.81515\nNormalized\n\n\n7\n175\n54.78088\nNormalized\n\n\n8\n225\n56.20738\nNormalized\n\n\n10\n235\n55.86135\nNormalized\n\n\n11\n50\n69.20071\nNone\n\n\n12\n25\n88.95808\nNone\n\n\n13\n75\n68.87041\nNone\n\n\n14\n100\n66.95614\nNone\n\n\n15\n150\n69.73371\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 28: Kaggle Best Scores\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 3.5))\nax = sns.barplot(df, x=\"Epoch\", y=\"Score\", hue=\"Transform\")\nfor container in ax.containers:\n    ax.bar_label(container, fmt=\"%.1f\") # type: ignore\nplt.ylabel(\"Kaggle MiFID Score\");\nplt.ylim(0, 105)\n\n\n\n\n\n\n\n\nFigure 29: Kaggle Scores Comparison (Lower is Better)\n\n\n\n\n\n\nAs visualized in Figure 29 our GAN architecture was evaluated across multiple training configurations, with the primary comparison between normalized and non-normalized input data. Training proceeded for 250 epochs, implementing both adversarial and cycle consistency losses to balance image quality and style transfer.\nThe normalized data approach demonstrated superior performance, achieving the best MiFID score of 53.82 at epoch 200, compared to the best non-normalized score of 66.96 at epoch 100. This aligns with our initial data analysis, which identified distinct color distributions between Monet paintings and photographs. The normalization process helped the model focus on learning stylistic features rather than managing color distribution differences."
  },
  {
    "objectID": "index.html#model-evolution",
    "href": "index.html#model-evolution",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "6.5 Model Evolution",
    "text": "6.5 Model Evolution\nTraining progression showed several key patterns:\n\nEarly epochs (1-50) exhibited rapid improvement in style transfer\nMid-range epochs (50-150) demonstrated refinement in color palette and brush stroke effects\nLater epochs (150-250) showed diminishing returns in quality improvement\n\nThe normalized data configuration showed more stable training behavior, with MiFID scores consistently improving until epoch 200. In contrast, non-normalized training showed more volatility in scores, suggesting less stable learning dynamics."
  },
  {
    "objectID": "index.html#additional-hyperparameter-considerations",
    "href": "index.html#additional-hyperparameter-considerations",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "7.1 Additional Hyperparameter Considerations",
    "text": "7.1 Additional Hyperparameter Considerations\nWhile our implementation focused on normalization strategies, several key hyperparameters could be tuned for potentially better performance:\n\nLearning rate scheduling: Implementing decay schedules could improve convergence\nLoss function weights: Adjusting the balance between cycle consistency (currently 10x) and identity losses (currently 5x)\nBatch size optimization: Exploring larger batch sizes to stabilize training\nNetwork architecture parameters: Testing different numbers of residual blocks and filter sizes\nOptimizer parameters: Fine-tuning Adam’s beta values (currently β₁=0.5, β₂=0.999)"
  },
  {
    "objectID": "index.html#limitations-and-future-work",
    "href": "index.html#limitations-and-future-work",
    "title": "DTSA 5511 Introduction to Machine Learning: Deep Learning",
    "section": "7.2 Limitations and Future Work",
    "text": "7.2 Limitations and Future Work\nWhile our model achieved promising results, several areas merit further investigation:\n\nExploration of alternative normalization strategies\nInvestigation of deeper residual architectures\nIncorporation of attention mechanisms for improved style transfer\n\nThe success of our normalized training approach suggests that careful consideration of data preprocessing strategies is crucial for effective style transfer applications."
  }
]